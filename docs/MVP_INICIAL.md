# AutoU ‚Äì Classificador & Respostas de E‚Äëmails (MVP)

Abaixo segue um **pacote completo** (estrutura de reposit√≥rio, c√≥digo, requisitos e guias de deploy + roteiro de v√≠deo)** pronto para voc√™ colar no GitHub**. √â um MVP em **FastAPI + HTML/Tailwind + Alpine.js**, com **Transformers (zero‚Äëshot)** para classifica√ß√£o e **gera√ß√£o de resposta** via **templates** ou **OpenAI** (opcional, se houver chave), al√©m de **parser de PDF**.

---

## üìÅ Estrutura de Pastas
```
autou-email-ai/
‚îú‚îÄ app/
‚îÇ  ‚îú‚îÄ main.py
‚îÇ  ‚îú‚îÄ nlp.py
‚îÇ  ‚îú‚îÄ responders.py
‚îÇ  ‚îú‚îÄ utils.py
‚îÇ  ‚îú‚îÄ models/
‚îÇ  ‚îÇ  ‚îî‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ templates/
‚îÇ  ‚îÇ  ‚îî‚îÄ index.html
‚îÇ  ‚îî‚îÄ static/
‚îÇ     ‚îú‚îÄ styles.css
‚îÇ     ‚îî‚îÄ app.js
‚îú‚îÄ sample_emails/
‚îÇ  ‚îú‚îÄ produtivo_status.txt
‚îÇ  ‚îú‚îÄ produtivo_anexo.txt
‚îÇ  ‚îî‚îÄ improdutivo_felicitacao.txt
‚îú‚îÄ tests/
‚îÇ  ‚îî‚îÄ test_api.py
‚îú‚îÄ deploy/
‚îÇ  ‚îú‚îÄ .env.example
‚îÇ  ‚îú‚îÄ deploy.sh
‚îÇ  ‚îî‚îÄ README.md
‚îú‚îÄ requirements.txt
‚îú‚îÄ Dockerfile
‚îú‚îÄ render.yaml
‚îú‚îÄ Procfile
‚îú‚îÄ README.md
‚îî‚îÄ LICENSE
```

---

## ‚úÖ Requisitos & Decis√µes T√©cnicas (resumo)
- **Classifica√ß√£o**: `transformers` com *zero-shot* usando **joeddav/xlm-roberta-large-xnli** (multil√≠ngue, √≥timo para PT‚ÄëBR) e r√≥tulos: `Produtivo` vs `Improdutivo` + detec√ß√£o de **inten√ß√£o** (status, envio de arquivo, d√∫vida t√©cnica, agradecimento/felicit.)
- **Gera√ß√£o de resposta**:
  - *Default:* **templates** condicionais por categoria/inten√ß√µes, com placeholders (n¬∫ do ticket, anexos, SLA, etc.).
  - *Opcional:* se `OPENAI_API_KEY` definido, usa **OpenAI** para refinar o texto com tom profissional e RGPD/LGPD‚Äëfriendly.
- **Pr√©-processamento NLP**: normaliza√ß√£o, remo√ß√£o de *stopwords* PT‚ÄëBR, lematiza√ß√£o (opcional via spaCy) e heur√≠sticas para detec√ß√£o de anexos/pedidos de status.
- **Uploads**: aceita `.txt` e `.pdf` (via `pdfminer.six`).
- **UI**: HTML + Tailwind (CDN) + Alpine.js. Drag & drop, colar texto, **pr√©vias** de texto, **badge** de categoria, **c√≥pia r√°pida** da resposta.
- **Backend**: FastAPI, endpoints `/api/process` e `/health`. Respostas em JSON. CORS pronto.
- **Deploy**:
  - **AWS Lambda** (recomendado): Serverless Framework + `serverless.yml`
  - **Render** (legado): `render.yaml` + `Procfile`
  - **Hugging Face Spaces** (Gradio opcional) **ou** qualquer servi√ßo com Dockerfile.
- **Testes**: `pytest` cobre sucesso de `/api/process` e classifica√ß√£o b√°sica.

---

## üîê Vari√°veis de Ambiente (deploy/.env.example)
```
# Opcional ‚Äì refino de respostas por LLM
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# Modelo de zero-shot (padr√£o multil√≠ngue)
ZSL_MODEL=joeddav/xlm-roberta-large-xnli

# FastAPI
PORT=8000
HOST=0.0.0.0

# Limites
MAX_TEXT_CHARS=20000
```

> Se `OPENAI_API_KEY` n√£o existir, o sistema usa apenas **templates** ‚Äì ainda assim voc√™ cumpre o desafio 100%.

---

## üêç `requirements.txt`
```
fastapi==0.115.0
uvicorn[standard]==0.30.6
python-multipart==0.0.9
jinja2==3.1.4
pydantic==2.9.2
pydantic-settings==2.5.2
transformers==4.44.2
torch==2.4.0
nltk==3.9.1
pdfminer.six==20231228
openai==1.52.2
spacy==3.7.4
spacy-lookups-data==1.0.5
```

> **Opcional**: baixar `pt_core_news_sm` para lematiza√ß√£o (`python -m spacy download pt_core_news_sm`).

---

## üß† `app/nlp.py`
```python
import os
from typing import Dict, List
import nltk
from nltk.corpus import stopwords
from transformers import pipeline

# NLTK setup
nltk.download("stopwords", quiet=True)
STOP_PT = set(stopwords.words("portuguese"))

ZSL_MODEL = os.getenv("ZSL_MODEL", "joeddav/xlm-roberta-large-xnli")

# Lazy init (carrega uma vez)
_zsl_cls = None
_intent_cls = None

LABELS_CATEGORY = ["Produtivo", "Improdutivo"]
LABELS_INTENT = [
    "Solicita√ß√£o de status",
    "Envio de documentos/arquivo",
    "D√∫vida t√©cnica",
    "Agradecimento/Felicitacao",
    "Spam/Irrelevante"
]


def get_classifier():
    global _zsl_cls
    if _zsl_cls is None:
        _zsl_cls = pipeline("zero-shot-classification", model=ZSL_MODEL)
    return _zsl_cls


def preprocess(text: str) -> str:
    # minify
    text = (text or "").strip()
    # remove stopwords simples mantendo sem√¢ntica
    tokens = [t for t in text.split() if t.lower() not in STOP_PT]
    return " ".join(tokens) if tokens else text


def classify_email(text: str) -> Dict:
    clf = get_classifier()
    processed = preprocess(text)

    # Categoria (bin√°ria)
    cat = clf(processed, LABELS_CATEGORY, multi_label=False)
    category = cat["labels"][0]
    cat_score = float(cat["scores"][0])

    # Inten√ß√£o (top‚Äë1)
    intent = clf(processed, LABELS_INTENT, multi_label=False)
    top_intent = intent["labels"][0]
    intent_score = float(intent["scores"][0])

    return {
        "category": category,
        "category_score": cat_score,
        "intent": top_intent,
        "intent_score": intent_score,
        "processed": processed,
    }
```

---

## ‚úâÔ∏è `app/responders.py`
```python
import os
from typing import Dict
from datetime import datetime

from openai import OpenAI

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# === Templates base (portugu√™s formal corporativo) ===
TEMPLATES = {
    ("Produtivo", "Solicita√ß√£o de status"): (
        "Assunto: Atualiza√ß√£o do seu atendimento\n\n"
        "Ol√°, {nome}, tudo bem?\n\n"
        "Localizamos sua solicita√ß√£o {referencia}. No momento, ela est√° em '{status_atual}'.\n"
        "Previs√£o de pr√≥xima atualiza√ß√£o: {sla}.\n\n"
        "Se houver qualquer novo documento ou informa√ß√£o, por gentileza responda a este e-mail.\n\n"
        "Atenciosamente,\nEquipe de Suporte"
    ),
    ("Produtivo", "Envio de documentos/arquivo"): (
        "Assunto: Documentos recebidos com sucesso\n\n"
        "Ol√°, {nome}. Confirmamos o recebimento do(s) arquivo(s): {arquivos}.\n"
        "Encaminhamos para an√°lise e retornamos at√© {sla}.\n\n"
        "Atenciosamente,\nEquipe de Suporte"
    ),
    ("Produtivo", "D√∫vida t√©cnica"): (
        "Assunto: Retorno sobre sua d√∫vida t√©cnica\n\n"
        "Ol√°, {nome}. Obrigado por nos contatar.\n"
        "Para agilizar, poderia informar: {perguntas_faltantes}?\n"
        "Assim que recebermos, seguimos com a solu√ß√£o. Prazo estimado: {sla}.\n\n"
        "Atenciosamente,\nEquipe de Suporte"
    ),
    ("Improdutivo", "Agradecimento/Felicitacao"): (
        "Assunto: Agradecemos a sua mensagem\n\n"
        "Ol√°, {nome}! Muito obrigado pela sua mensagem.\n"
        "Ficamos √† disposi√ß√£o caso precise de algo.\n\n"
        "Abra√ßos,\nEquipe"
    ),
    ("Improdutivo", "Spam/Irrelevante"): (
        "Assunto: Confirma√ß√£o de recebimento\n\n"
        "Ol√°. Sua mensagem foi recebida. Caso necessite suporte, por favor descreva o assunto e um identificador (ex.: n¬∫ de contrato/atendimento).\n\n"
        "Atenciosamente,\nEquipe"
    ),
}


def _fill(template: str, ctx: Dict) -> str:
    defaults = dict(
        nome="",
        referencia="(ID n√£o informado)",
        status_atual="em an√°lise",
        sla=datetime.utcnow().strftime("%d/%m/%Y"),
        arquivos="(n√£o especificado)",
        perguntas_faltantes="ambiente, passos para reproduzir e prints/logs",
    )
    defaults.update({k: v for k, v in ctx.items() if v})
    try:
        return template.format(**defaults)
    except Exception:
        return template


def suggest_reply(category: str, intent: str, context: Dict) -> Dict:
    base = TEMPLATES.get((category, intent))
    if not base:
        # fallback
        base = (
            "Assunto: Retorno da sua mensagem\n\n"
            "Ol√°. Recebemos seu contato referente a '{intent}'. Em breve retornaremos.\n\n"
            "Atenciosamente, Equipe"
        )
        base = base.replace("{intent}", intent)

    filled = _fill(base, context or {})

    # Se houver OpenAI, refinamos o tom
    if OPENAI_API_KEY:
        client = OpenAI()
        prompt = (
            "Revise e melhore a mensagem abaixo com tom profissional e claro, mantendo o conte√∫do.\n\n"
            f"Mensagem:\n{filled}\n\nSa√≠da final apenas com o texto revisado."
        )
        try:
            resp = client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
            )
            improved = resp.choices[0].message.content.strip()
            return {"reply": improved, "source": "openai+template"}
        except Exception:
            pass

    return {"reply": filled, "source": "template"}
```

---

## üß∞ `app/utils.py`
```python
import io
from typing import Tuple
from pdfminer.high_level import extract_text

ALLOWED_EXTS = {".txt", ".pdf"}


def read_text_from_file(filename: str, content: bytes) -> Tuple[str, str]:
    name = filename or ""
    lower = name.lower()
    text = ""

    if lower.endswith(".txt"):
        text = content.decode("utf-8", errors="ignore")
        return text, "text/plain"
    if lower.endswith(".pdf"):
        # pdfminer precisa de um buffer
        with io.BytesIO(content) as pdf_buf:
            text = extract_text(pdf_buf) or ""
        return text, "application/pdf"
    raise ValueError("Formato de arquivo n√£o suportado. Use .txt ou .pdf.")
```

---

## üöÄ `app/main.py`
```python
import os
from fastapi import FastAPI, UploadFile, Form, File
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from app.utils import read_text_from_file
from app.nlp import classify_email
from app.responders import suggest_reply

PORT = int(os.getenv("PORT", 8000))
MAX_CHARS = int(os.getenv("MAX_TEXT_CHARS", 20000))

app = FastAPI(title="AutoU Email Classifier")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.mount("/static", StaticFiles(directory="app/static"), name="static")


class ProcessResponse(BaseModel):
    category: str
    category_score: float
    intent: str
    intent_score: float
    suggested_reply: str
    reply_source: str


@app.get("/", response_class=HTMLResponse)
def index():
    from fastapi.templating import Jinja2Templates
    from fastapi import Request
    templates = Jinja2Templates(directory="app/templates")
    return templates.TemplateResponse("index.html", {"request": Request(scope={"type": "http"})})


@app.get("/health")
def health():
    return {"status": "ok"}


@app.post("/api/process", response_model=ProcessResponse)
async def process_email(file: UploadFile | None = File(default=None), text: str | None = Form(default=None)):
    raw = ""
    filename = None
    if file is not None:
        filename = file.filename
        content = await file.read()
        raw, _ = read_text_from_file(filename, content)
    elif text:
        raw = text
    else:
        return JSONResponse({"detail": "Envie um arquivo .txt/.pdf ou cole o texto."}, status_code=400)

    if not raw.strip():
        return JSONResponse({"detail": "Conte√∫do vazio ap√≥s leitura."}, status_code=400)

    if len(raw) > MAX_CHARS:
        raw = raw[:MAX_CHARS]

    clf = classify_email(raw)

    # Contexto simples (poderia extrair com regex/NER)
    context = {
        "nome": "",
        "referencia": "",
        "status_atual": "em an√°lise",
        "sla": "2 dias √∫teis",
        "arquivos": filename or "",
        "perguntas_faltantes": "ambiente, passos para reproduzir, prints/logs",
    }

    reply = suggest_reply(clf["category"], clf["intent"], context)

    return ProcessResponse(
        category=clf["category"],
        category_score=clf["category_score"],
        intent=clf["intent"],
        intent_score=clf["intent_score"],
        suggested_reply=reply["reply"],
        reply_source=reply["source"],
    )


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=PORT)
```

---

## üñ•Ô∏è `app/templates/index.html`
```html
<!doctype html>
<html lang="pt-BR">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>AutoU ‚Äì Classificador de E‚Äëmail</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <script defer src="https://unpkg.com/alpinejs@3.x.x/dist/cdn.min.js"></script>
  <link rel="stylesheet" href="/static/styles.css" />
</head>
<body class="bg-slate-50 text-slate-900">
  <main class="max-w-4xl mx-auto p-6" x-data="emailApp()">
    <header class="mb-6">
      <h1 class="text-2xl font-bold">AutoU ‚Äì Classificador & Respostas</h1>
      <p class="text-sm text-slate-600">Envie um .txt/.pdf ou cole o texto. O sistema identifica <b>Produtivo</b> vs <b>Improdutivo</b> e sugere uma resposta.</p>
    </header>

    <section class="grid gap-4 md:grid-cols-2">
      <div class="bg-white rounded-xl shadow p-4">
        <h2 class="font-semibold mb-2">Entrada</h2>
        <div class="space-y-3">
          <textarea x-model="rawText" class="w-full h-40 border rounded-lg p-3 focus:outline-none" placeholder="Cole o texto do e‚Äëmail aqui..."></textarea>
          <div class="flex items-center gap-2">
            <input type="file" x-ref="file" class="hidden" @change="onFileChange" accept=".txt,.pdf" />
            <button class="btn" @click="$refs.file.click()">Selecionar arquivo (.txt/.pdf)</button>
            <button class="btn-secondary" @click="clearAll">Limpar</button>
          </div>
          <button class="btn-primary w-full" @click="submit" :disabled="loading">
            <span x-show="!loading">Classificar & Sugerir Resposta</span>
            <span x-show="loading">Processando...</span>
          </button>
        </div>
      </div>

      <div class="bg-white rounded-xl shadow p-4">
        <h2 class="font-semibold mb-2">Resultado</h2>
        <template x-if="result">
          <div class="space-y-3">
            <div class="flex items-center gap-2">
              <span class="badge" :class="result.category==='Produtivo' ? 'bg-emerald-100 text-emerald-700' : 'bg-slate-200 text-slate-700'" x-text="result.category"></span>
              <span class="text-xs text-slate-500">confian√ßa: <span x-text="(result.category_score*100).toFixed(1)+'%'"></span></span>
            </div>
            <div class="text-sm text-slate-700">Inten√ß√£o: <b x-text="result.intent"></b> (<span x-text="(result.intent_score*100).toFixed(1)+'%'"></span>)</div>
            <div>
              <label class="text-sm text-slate-600">Resposta sugerida</label>
              <textarea class="w-full h-48 border rounded-lg p-3" x-model="result.suggested_reply"></textarea>
              <div class="flex gap-2 mt-2">
                <button class="btn" @click="copyReply">Copiar</button>
                <span class="text-xs text-slate-500" x-text="'Fonte: ' + (result.reply_source||'-')"></span>
              </div>
            </div>
          </div>
        </template>
        <template x-if="!result">
          <p class="text-sm text-slate-500">Nenhum resultado ainda. Fa√ßa o upload ou cole um texto e clique em <b>Classificar</b>.</p>
        </template>
      </div>
    </section>

    <footer class="mt-6 text-xs text-slate-500">
      <p>üõà Dica: sem <code>OPENAI_API_KEY</code>, o sistema usa templates. Com a chave, ele refina a resposta automaticamente.</p>
    </footer>
  </main>

  <script src="/static/app.js"></script>
</body>
</html>
```

---

## üé® `app/static/styles.css`
```css
.btn { @apply inline-flex items-center justify-center px-3 py-2 rounded-lg border border-slate-300 text-sm hover:bg-slate-50; }
.btn-secondary { @apply inline-flex items-center justify-center px-3 py-2 rounded-lg bg-slate-100 text-slate-700 text-sm hover:bg-slate-200; }
.btn-primary { @apply inline-flex items-center justify-center px-3 py-2 rounded-lg bg-indigo-600 text-white text-sm hover:bg-indigo-700 disabled:opacity-60; }
.badge { @apply inline-flex items-center px-2 py-1 rounded-full text-xs font-medium; }
```

---

## üß© `app/static/app.js`
```javascript
function emailApp(){
  return {
    rawText: "",
    file: null,
    loading: false,
    result: null,
    onFileChange(e){ this.file = e.target.files[0] || null; },
    clearAll(){ this.rawText = ""; this.file = null; this.result = null; },
    async submit(){
      this.loading = true; this.result = null;
      try{
        const form = new FormData();
        if(this.file){ form.append('file', this.file); }
        if(this.rawText && !this.file){ form.append('text', this.rawText); }
        const res = await fetch('/api/process', { method:'POST', body: form });
        if(!res.ok){ const e = await res.json(); throw new Error(e.detail || 'Erro'); }
        this.result = await res.json();
      }catch(err){
        alert(err.message);
      }finally{
        this.loading = false;
      }
    },
    async copyReply(){
      if(this.result?.suggested_reply){
        await navigator.clipboard.writeText(this.result.suggested_reply);
      }
    }
  }
}
```

---

## üß™ `tests/test_api.py`
```python
import json
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_health():
    r = client.get('/health')
    assert r.status_code == 200


def test_process_with_text():
    payload = {"text": "Ol√°, podem me passar o status do chamado 123?"}
    r = client.post('/api/process', data=payload)
    assert r.status_code == 200
    data = r.json()
    assert data["category"] in ("Produtivo", "Improdutivo")
    assert isinstance(data["suggested_reply"], str)
```

---

## üìù `README.md` (cole no repo)
```markdown
# AutoU ‚Äì Classificador & Respostas de E‚Äëmails (MVP)

MVP full‚Äëstack para **classificar e-mails** (Produtivo/Improdutivo), identificar **inten√ß√£o** e **sugerir respostas autom√°ticas**. Constru√≠do com **FastAPI** + **Transformers (zero-shot)** + **UI em HTML/Tailwind/Alpine**. Suporta **.txt** e **.pdf**.

## ‚ú® Funcionalidades
- Upload de `.txt`/`.pdf` ou colagem de texto
- Classifica√ß√£o **Produtivo vs Improdutivo** e **detec√ß√£o de inten√ß√£o**
- Resposta autom√°tica por **templates** (sem custo) ou refinada por **OpenAI** (opcional)
- UI leve e responsiva, com c√≥pia r√°pida de resposta

## üß∞ Stack
- Backend: FastAPI, Transformers (modelo `joeddav/xlm-roberta-large-xnli`)
- UI: HTML + Tailwind (CDN) + Alpine.js
- PDF: pdfminer.six
- Testes: pytest

## ‚öôÔ∏è Setup Local
```bash
python -m venv .venv && source .venv/bin/activate  # (Windows: .venv\\Scripts\\activate)
pip install -r requirements.txt
# (opcional) python -m spacy download pt_core_news_sm
cp deploy/.env.example .env
uvicorn app.main:app --reload --port 8000
```
Acesse: http://localhost:8000

## üîë OpenAI (opcional)
No `.env`, defina `OPENAI_API_KEY` para o refinamento das respostas. Sem a chave, o sistema usa templates.

## üß™ Testes
```bash
pytest -q
```

## ‚òÅÔ∏è Deploy (AWS Lambda - Recomendado)

1. **Configurar AWS CLI**: `aws configure`
2. **Instalar Serverless**: `npm install -g serverless`
3. **Deploy**: `cd deploy && npx serverless deploy`

**Status atual**: ‚úÖ Funcionando em `https://x1r6i3udxg.execute-api.us-east-1.amazonaws.com/dev/`

## ‚òÅÔ∏è Deploy (Render.com - Legado)
1. Fa√ßa **fork** deste reposit√≥rio.
2. No Render: **New + Web Service** ‚Üí conecte ao GitHub ‚Üí *Environment*: `Python` ‚Üí *Build Command*: `pip install -r requirements.txt` ‚Üí *Start Command*: `uvicorn app.main:app --host 0.0.0.0 --port 10000` (ou use `Procfile`).
3. Defina env vars (opcional): `OPENAI_API_KEY`, `ZSL_MODEL`.
4. Publique e copie a URL p√∫blica.

> Alternativa: **Docker**
```bash
docker build -t autou-email-ai .
docker run -p 8000:8000 autou-email-ai
```

## ‚òÅÔ∏è Deploy (Hugging Face Spaces ‚Äì Docker)
- Crie um Space tipo **Docker** e envie os arquivos (`Dockerfile`, `requirements.txt`, etc.).

## üì¶ Endpoints
- `GET /` ‚Üí UI
- `POST /api/process` ‚Üí recebe `file` (.txt/.pdf) **ou** `text` (form)
- `GET /health`

### Exemplo de resposta `/api/process`
```json
{
  "category": "Produtivo",
  "category_score": 0.94,
  "intent": "Solicita√ß√£o de status",
  "intent_score": 0.88,
  "suggested_reply": "Assunto: ...",
  "reply_source": "template"
}
```

## üìÇ Dados de Exemplo
Consulte a pasta `sample_emails/`.

## üîç Notas de Qualidade
- C√≥digo **organizado**, **tipado** e com **docstrings**.
- Pipeline `zero-shot` evita treinar do zero, mas voc√™ pode **fine‚Äëtunar** futuramente.
- Para *privacy*: sem logs de dados por padr√£o; adicionar mascaramento de PII √© trivial.

## üõ°Ô∏è LGPD
- Respostas padronizadas n√£o exp√µem dados sens√≠veis.
- Se usar LLM externo (OpenAI), garanta contratos/DPA.

## üó∫Ô∏è Roadmap (extras)
- Extra√ß√£o de **ID de ticket** via regex/NER
- Dashboard de **m√©tricas** (acur√°cia, distribui√ß√£o por inten√ß√£o)
- Fila ass√≠ncrona (Celery/Redis) para alto volume
- Login SSO e feedback humano (aprendizado ativo)
```

---

## üßæ `Dockerfile`
```dockerfile
FROM python:3.11-slim

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

WORKDIR /app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY . .

EXPOSE 8000
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## üîß `Procfile`
```
web: uvicorn app.main:app --host 0.0.0.0 --port $PORT
```

---

## üìú `render.yaml`
```yaml
services:
  - type: web
    name: autou-email-ai
    env: python
    plan: free
    buildCommand: pip install -r requirements.txt
    startCommand: uvicorn app.main:app --host 0.0.0.0 --port $PORT
    envVars:
      - key: OPENAI_API_KEY
        sync: false
      - key: ZSL_MODEL
        value: joeddav/xlm-roberta-large-xnli
```

---

## üß™ `sample_emails/`
**produtivo_status.txt**
```
Bom dia, poderiam me informar o status do chamado #45721? Precisa de mais algum documento?
```

**produtivo_anexo.txt**
```
Segue em anexo o comprovante solicitado para dar sequ√™ncia no processo 8821.
```

**improdutivo_felicitacao.txt**
```
Feliz Natal a toda a equipe! Obrigado pelo suporte ao longo do ano.
```

---

## üé• Roteiro do V√≠deo (3‚Äì5 min)
**0:00 ‚Äì 0:30 | Introdu√ß√£o**
- Nome, contexto do desafio (empresa financeira, alto volume de e-mails) e objetivo (automatizar leitura, classifica√ß√£o e resposta).

**0:30 ‚Äì 3:30 | Demonstra√ß√£o**
- Acessa URL deployada, mostra upload de `.txt/.pdf` e colagem de texto.
- Exibe categoria (badge), inten√ß√£o e confian√ßa.
- Mostra resposta sugerida e bot√£o de copiar.
- (Opcional) Mostra diferen√ßa com/sem `OPENAI_API_KEY`.

**3:30 ‚Äì 4:30 | T√©cnica**
- FastAPI + zero-shot `joeddav/xlm-roberta-large-xnli`.
- Pr√©-processamento (stopwords), parser PDF, templates de resposta e refino com LLM.
- Decis√µes: zero‚Äëshot (sem treino), multil√≠ngue, extens√≠vel com NER/regex.

**4:30 ‚Äì 5:00 | Conclus√£o**
- Refor√ßa benef√≠cios: economia de tempo, consist√™ncia, base s√≥lida para escalar.
- Pr√≥ximos passos: dashboard, feedback humano, fine‚Äëtuning.

---

## üìå Observa√ß√µes finais
- Com esse pacote, voc√™ cobre **todos os entreg√°veis**: c√≥digo, README com instru√ß√µes e guias de deploy, amostras e roteiro do v√≠deo.
- Se quiser, posso **gerar um README j√° pronto em PT/EN** e um **script de grava√ß√£o** com bullet‚Äëpoints para leitura durante o v√≠deo.

